[project]
name = "gemma3-qa-finetuning"
version = "0.1.0"
description = "Full FT vs LoRA vs Freezing for extractive QA (SQuAD) with Gemma 3."
readme = "README.md"
requires-python = "==3.12.*"
dependencies = [
    "bs4>=0.0.2",
    "datasets>=4.4.1",
    "evaluate>=0.4.6",
    "faiss-cpu>=1.12.0",
    "langchain-community>=0.4.1",
    "langchain-huggingface>=1.1.0",
    "langchain-text-splitters>=1.0.0",
    "langchain[openai]>=1.0.7",
    "langgraph>=1.0.3",
    "peft>=0.18.0",
    "sentence-transformers>=5.1.2",
    "torch>=2.5.0",
    "transformers>=4.40.0",
    "bitsandbytes>=0.43.0",
]

[build-system]
requires = ["setuptools>=68"]
build-backend = "setuptools.build_meta"

[[tool.uv.index]]
name = "pytorch-cu124"
url = "https://download.pytorch.org/whl/cu124"
explicit = true

[tool.uv.sources]
torch = [
  { index = "pytorch-cu124" },
]

[tool.setuptools.packages.find]
where = ["src"]

[project.scripts]
gemmaqa-train = "gemmaqa.finetuning.train:main"
gemmaqa-eval  = "gemmaqa.inference.eval:main"
gemmaqa-chat  = "gemmaqa.inference.chat:main"
gemmaqa-check-cuda = "gemmaqa.utils.cuda_check:main"
