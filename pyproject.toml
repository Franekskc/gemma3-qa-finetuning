[project]
name = "gemma3-qa-finetuning"
version = "0.1.0"
description = "Full FT vs LoRA vs Freezing for extractive QA (SQuAD) with Gemma 3."
readme = "README.md"
requires-python = "==3.12.*"
dependencies = [
    "bs4>=0.0.2",
    "datasets>=4.4.1",
    "evaluate>=0.4.6",
    "peft>=0.18.0",
    "pyyaml>=6.0.3",
    "scikit-learn>=1.7.2",
    "sentence-transformers>=5.1.2",
    "bitsandbytes>=0.43.0",
    "torch>=2.6.0",
    "torch-optimi>=0.3.2",
    "transformers>=4.57.1",
    "structlog>=25.5.0",
    "faiss-cpu>=1.13.0",
]

[build-system]
requires = ["setuptools>=68"]
build-backend = "setuptools.build_meta"

[[tool.uv.index]]
name = "pytorch-cu124"
url = "https://download.pytorch.org/whl/cu124"
explicit = true

[tool.uv.sources]
torch = [
  { index = "pytorch-cu124" },
]

[tool.setuptools.packages.find]
where = ["src"]

[project.scripts]
gemmaqa-train = "gemmaqa.finetuning.train:main"
gemmaqa-eval  = "gemmaqa.inference.eval:main"
gemmaqa-chat  = "gemmaqa.inference.chat:main"
gemmaqa-check-cuda = "gemmaqa.utils.cuda_check:main"
gemmaqa-prepare-data = "gemmaqa.finetuning.prepare_data:main"
